{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cohere\n",
      "  Downloading cohere-5.14.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
      "  Downloading fastavro-1.10.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from cohere) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from cohere) (0.4.0)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from cohere) (2.10.3)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from cohere) (2.27.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from cohere) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from cohere) (0.21.0)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
      "  Downloading types_requests-2.32.0.20250328-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from cohere) (4.12.2)\n",
      "Requirement already satisfied: anyio in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from pydantic>=1.9.2->cohere) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from tokenizers<1,>=0.15->cohere) (0.29.3)\n",
      "Requirement already satisfied: filelock in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
      "Downloading cohere-5.14.0-py3-none-any.whl (253 kB)\n",
      "Downloading fastavro-1.10.0-cp312-cp312-macosx_10_13_universal2.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.0.20250328-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: types-requests, fastavro, cohere\n",
      "Successfully installed cohere-5.14.0 fastavro-1.10.0 types-requests-2.32.0.20250328\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U cohere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 48 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing your PDF documents\n",
    "documents_dir = \"../document/\"\n",
    "\n",
    "# Create a DirectoryLoader to load all PDF files from the directory\n",
    "loader = DirectoryLoader(\n",
    "    documents_dir,\n",
    "    glob=\"**/*.pdf\",  # This will load all PDF files recursively\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Load the documents\n",
    "documents = loader.load()\n",
    "\n",
    "# Print the number of documents loaded\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 217 chunks\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Print the number of chunks created\n",
    "print(f\"Created {len(texts)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qc/q94_1y514n3fvqnfz08sykkh0000gn/T/ipykernel_3000/1984403406.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/Users/uddeshyabarnwal/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 217 vectors in the database\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Print the number of vectors stored\n",
    "print(f\"Stored {len(texts)} vectors in the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\n",
      "computation [26], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in [{'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 1, 'page_label': '2', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': '../document/attention-is-all-you-need-Paper.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}]\n",
      "* we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "output values. These are concatenated and once again projected, resulting in the ﬁnal values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "variables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\n",
      "i=1 qiki, has mean 0 and variance dk.\n",
      "4 [{'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 3, 'page_label': '4', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': '../document/attention-is-all-you-need-Paper.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}]\n",
      "* The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions [{'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 1, 'page_label': '2', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': '../document/attention-is-all-you-need-Paper.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}]\n",
      "* during training.\n",
      "4 Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range\n",
      "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      "ability to learn such dependencies is the length of the paths forward and backward signals have to [{'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 5, 'page_label': '6', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': '../document/attention-is-all-you-need-Paper.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}]\n",
      "* Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser ∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly [{'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 0, 'page_label': '1', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': '../document/attention-is-all-you-need-Paper.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}]\n"
     ]
    }
   ],
   "source": [
    "results = vectorstore.similarity_search(\n",
    "    \"what are the main principle of Attention Block\",\n",
    "    k=5,\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def search_documents(\n",
    "    query: str,\n",
    "    vectorstore: Chroma,\n",
    "    k: int = 5\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Perform similarity search on the vector store using the provided query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query from the user\n",
    "        vectorstore (Chroma): The initialized Chroma vector store\n",
    "        k (int, optional): Number of results to return. Defaults to 5.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of page content from the documents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = vectorstore.similarity_search(\n",
    "            query,\n",
    "            k=k\n",
    "        )\n",
    "        return [doc.page_content for doc in results]\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing similarity search: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "COHERE_API_KEY = os.getenv('COHERE_API_KEY')\n",
    "co = cohere.ClientV2(COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document=None index=3 relevance_score=0.074966356\n",
      "document=None index=0 relevance_score=0.05086082\n",
      "document=None index=4 relevance_score=0.04248727\n"
     ]
    }
   ],
   "source": [
    "query = '''What are the main principles of Attention Block?'''\n",
    "context = search_documents(query, vectorstore, k=10)\n",
    "# Rerank the documents\n",
    "results = co.rerank(\n",
    "    model=\"rerank-v3.5\", query=query, documents=context, top_n=3\n",
    ")\n",
    "for result in results.results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in', 'of 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned']\n"
     ]
    }
   ],
   "source": [
    "reranked_context = []\n",
    "for result in results.results:\n",
    "    reranked_context.append(context[result.index])\n",
    "print(reranked_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Attention Block is built on several key ideas:\n",
      "\n",
      "1. Parallelized Dependency Modeling: Instead of processing tokens one by one (as in recurrent models), self-attention allows all positions in a sequence to be computed in parallel. Each token can directly interact with all other tokens, making it much easier to capture long-range dependencies.\n",
      "\n",
      "2. Scaled Dot-Product Attention: The core computation involves comparing “queries” with “keys” using a dot product to measure similarity, and then using the result to weight “value” vectors. Because large dot products (especially with high-dimensional keys) can push the softmax into regions with very small gradients, the dot products are scaled by 1/√(dk) (where dk is the dimensionality of the keys) to keep the gradients in a healthy range.\n",
      "\n",
      "3. Multi-Head Attention: Rather than performing a single attention operation, the model first projects the queries, keys, and values several times using different learned linear transformations. These separate attention “heads” capture different aspects or subspaces of the input. Their outputs are then combined to form a richer representation, allowing the model to flexibly capture diverse relationships between the tokens.\n",
      "\n",
      "Together, these principles allow the Attention Block to effectively relate information across the entire sequence (regardless of distance between tokens), support efficient parallel computations, and maintain robust learning of dependencies even when the underlying representations have high dimensionality.\n"
     ]
    }
   ],
   "source": [
    "systemPrompt = f'''You are an intelligent bot you'll be given a text and you'll have to answer the question based on the text\n",
    "{reranked_context}\n",
    "'''\n",
    "conversationHistory = [\n",
    "    {\"role\": \"system\", \"content\": systemPrompt},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\n",
    "def Answer_Question(conversationHistory):\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"o3-mini\",\n",
    "        messages=conversationHistory,\n",
    "        \n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "response = Answer_Question(conversationHistory)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
